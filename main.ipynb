{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T05:53:08.053935Z",
     "start_time": "2024-04-29T05:53:07.970998Z"
    }
   },
   "id": "858d79a0bfc785a1",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 总览\n",
    "本次作业实现了 LyuTorch，其实现了一个 Tensor 类，这是一个 numpy.ndarray 的子类，在这个类中，作者手动实现了反向求导的功能.\n",
    "\n",
    "Tensor 类支持对少数操作的反向求导，足以构造一个只包含全链接层的神经网络，方法十分简单.\n",
    "\n",
    "```pycon\n",
    ">>> import lyutorch as lyu\n",
    ">>> a = lyu.tensor([[1, 2, 3]], requires_grad=True)\n",
    ">>> c = a @ a.t()\n",
    ">>> c = lyu.squeeze(c)\n",
    ">>> c.backward()\n",
    ">>> print(a.grad)\n",
    "[[1. 2. 3.]]\n",
    "```\n",
    "上方的例子展示了如何使用 LyuTorch 进行反向求导，该例子计算了一个向量和自身的点乘. 这里的 c = a @ a.t() 是矩阵乘法，squeeze 是去掉长度为 1 的维度，backward 是反向求导.\n",
    "\n",
    "以下是本次作业的主要代码：\n",
    "## 读取数据"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c84ef9ce3bcebd7a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-29T05:53:08.257441Z",
     "start_time": "2024-04-29T05:53:08.056475Z"
    }
   },
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def read_file(folder_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    此函数从文件夹下读取所有形如idxn-ubyte的数据文件.\n",
    "    :param folder_name: 文件夹字符串名.\n",
    "    :return: 包含所有以文件名（不含后缀名）为key，ndarray为value的字典.\n",
    "    \"\"\"\n",
    "    # 检查输入的文件夹是否存在，是否可读\n",
    "    if not os.path.exists(folder_name):\n",
    "        raise FileNotFoundError(f\"Folder doesn't exists: {folder_name}\")\n",
    "    if not os.access(folder_name, os.R_OK):\n",
    "        raise PermissionError(f\"Folder not readable: {folder_name}\")\n",
    "\n",
    "    return_dict = {}\n",
    "    for elements in os.scandir(folder_name):\n",
    "        if elements.is_file():\n",
    "            with open(elements.path, 'rb') as file:\n",
    "                magic, size = struct.unpack('>II', file.read(8))\n",
    "                dimension = magic & 0xFF  # 使用位运算获取存储数据的维数\n",
    "                lengths = (size,) + struct.unpack(f\">{dimension - 1}I\",\n",
    "                                                  file.read((dimension - 1) * 4))  #定义期望得到的ndarray结构\n",
    "                data_temp = np.fromfile(file, dtype=np.uint8).reshape(lengths)  # 读取二进制文件并重整成正确的结构\n",
    "                return_dict[os.path.splitext(elements.name)[0]] = data_temp\n",
    "    return return_dict\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "files = read_file('./dataset')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据预处理\n",
    "接下来我们对数据做最简单的预处理，首先将数据转化为我们自己实现的 Tensor 格式，之后将数据归一化到 $[-1, 1]$ 区间. 同时，我们将所有的 index 变成 one-hot 编码. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39b1bc7ad37fbdb3"
  },
  {
   "cell_type": "code",
   "source": [
    "import lyutorch as lyu\n",
    "img_train = lyu.tensor(files['train-images'] * 2 / 255 - 1, dtype=np.float32)\n",
    "img_test = lyu.tensor(files['t10k-images'] * 2 / 255 - 1, dtype=np.float32)\n",
    "label_train = lyu.tensor(files['train-labels'], dtype=np.int8)\n",
    "label_test = lyu.tensor(files['t10k-labels'], dtype=np.int8)\n",
    "\n",
    "# 将数据变成 one-hot 编码\n",
    "one_hot_matrix = np.eye(10)\n",
    "label_train = lyu.tensor(one_hot_matrix[label_train.flatten()].reshape(-1,10).squeeze())\n",
    "label_test = lyu.tensor(one_hot_matrix[label_test.flatten()].reshape(-1,10).squeeze())\n",
    "\n",
    "print(label_train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T05:53:09.511443Z",
     "start_time": "2024-04-29T05:53:08.258426Z"
    }
   },
   "id": "bf9eca7b984d1bc7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 网络结构\n",
    "我们按照作业要求，使用三层线性神经网络对 MNIST 数据集进行预测，其中第一层和第二层的激活函数为 ReLU，第三层输出前的激活函数为 softmax."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a4c59cb6d696638"
  },
  {
   "cell_type": "code",
   "source": [
    "import lyutorch.nn as nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_layer_size: int = 64):\n",
    "        super(Net, self).__init__()\n",
    "        self._hidden = hidden_layer_size\n",
    "        self.fc1 = lyu.tensor(np.random.normal(size=(28 * 28, hidden_layer_size)), requires_grad=True)\n",
    "        self.fc2 = lyu.tensor(np.random.normal(size=(hidden_layer_size, 10)), requires_grad=True)\n",
    "        self.bias1 = lyu.tensor(np.random.normal(size=(1, hidden_layer_size)), requires_grad=True)\n",
    "        self.bias2 = lyu.tensor(np.random.normal(size=(1, 10)), requires_grad=True)\n",
    "        self.add_parameter('fc1', self.fc1)\n",
    "        self.add_parameter('fc2', self.fc2)\n",
    "        self.add_parameter('bias1', self.bias1)\n",
    "        self.add_parameter('bias2', self.bias2)\n",
    "        self.target = lyu.tensor(np.eye(10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x @ self.fc1\n",
    "        x = x + self.bias1\n",
    "        x = nn.functional.relu(x)\n",
    "        x = x @ self.fc2\n",
    "        x = x + self.bias2\n",
    "        x = nn.functional.softmax(x, self.target)\n",
    "        return x\n",
    "\n",
    "    def l2normalization(self):\n",
    "        return (lyu.linalg.norm2(self.fc1, order=2) + lyu.linalg.norm2(self.fc2, order=2) + lyu.linalg.norm2(\n",
    "            self.bias1, order=2) + lyu.linalg.norm2(self.bias2, order=2)) * lyu.tensor(\n",
    "            1 / (28 * 28 * self._hidden + self._hidden * 10 + 10 + self._hidden))\n",
    "\n",
    "    def grad_off(self):\n",
    "        for key, value in self.parameters():\n",
    "            value.requires_grad = False\n",
    "\n",
    "    def grad_on(self):\n",
    "        for key, value in self.parameters():\n",
    "            value.requires_grad = True\n",
    "\n",
    "\n",
    "img_train = img_train.reshape((60000, 28 * 28))\n",
    "img_test = img_test.reshape((10000, 28 * 28))\n",
    "# 创建网络实例\n",
    "net = Net(hidden_layer_size=64)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T05:53:09.621659Z",
     "start_time": "2024-04-29T05:53:09.513442Z"
    }
   },
   "id": "e901e5cb43c0d53",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 实际训练",
   "id": "b6dbc1b9a6f14b70"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:53:09.730670Z",
     "start_time": "2024-04-29T05:53:09.624850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "net = Net(hidden_layer_size=32)\n",
    "sgd = nn.Optimizer(net.parameters(), lr=0.01)\n",
    "sgd.zero_grad()\n",
    "losses32 = []\n",
    "BATCH_SIZE = 32\n",
    "data = nn.DataLoader([img_train, label_train], shuffle=True, batch_size=BATCH_SIZE)\n"
   ],
   "id": "bf17fcd1ac86a512",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 76.834783  [   32/60000]\n",
      "loss: 84.136834  [   64/60000]\n",
      "loss: 89.648637  [   96/60000]\n",
      "loss: 47.853728  [  128/60000]\n",
      "loss: 64.910698  [  160/60000]\n",
      "loss: 71.490694  [  192/60000]\n",
      "loss: 55.311611  [  224/60000]\n",
      "loss: 60.993921  [  256/60000]\n",
      "loss: 51.275977  [  288/60000]\n",
      "loss: 54.094966  [  320/60000]\n",
      "loss: 45.806315  [  352/60000]\n",
      "loss: 49.016111  [  384/60000]\n",
      "loss: 39.982277  [  416/60000]\n",
      "loss: 37.065085  [  448/60000]\n",
      "loss: 36.853471  [  480/60000]\n",
      "loss: 29.500340  [  512/60000]\n",
      "loss: 40.187281  [  544/60000]\n",
      "loss: 31.480264  [  576/60000]\n",
      "loss: 30.442580  [  608/60000]\n",
      "loss: 37.237446  [  640/60000]\n",
      "loss: 23.733061  [  672/60000]\n",
      "loss: 30.451479  [  704/60000]\n",
      "loss: 34.108108  [  736/60000]\n",
      "loss: 34.846753  [  768/60000]\n",
      "loss: 35.966299  [  800/60000]\n",
      "loss: 21.968904  [  832/60000]\n",
      "loss: 29.538066  [  864/60000]\n",
      "loss: 21.042049  [  896/60000]\n",
      "loss: 26.515570  [  928/60000]\n",
      "loss: 21.909045  [  960/60000]\n",
      "loss: 29.540364  [  992/60000]\n",
      "loss: 22.124094  [ 1024/60000]\n",
      "loss: 24.426970  [ 1056/60000]\n",
      "loss: 28.342553  [ 1088/60000]\n",
      "loss: 25.552753  [ 1120/60000]\n",
      "loss: 37.488780  [ 1152/60000]\n",
      "loss: 22.359832  [ 1184/60000]\n",
      "loss: 27.011398  [ 1216/60000]\n",
      "loss: 25.366797  [ 1248/60000]\n",
      "loss: 29.688101  [ 1280/60000]\n",
      "loss: 23.608657  [ 1312/60000]\n",
      "loss: 29.648458  [ 1344/60000]\n",
      "loss: 23.183726  [ 1376/60000]\n",
      "loss: 25.008003  [ 1408/60000]\n",
      "loss: 25.071625  [ 1440/60000]\n",
      "loss: 23.574822  [ 1472/60000]\n",
      "loss: 25.600337  [ 1504/60000]\n",
      "loss: 27.362782  [ 1536/60000]\n",
      "loss: 21.032267  [ 1568/60000]\n",
      "loss: 24.148601  [ 1600/60000]\n",
      "loss: 19.799189  [ 1632/60000]\n",
      "loss: 15.198780  [ 1664/60000]\n",
      "loss: 18.765878  [ 1696/60000]\n",
      "loss: 22.260228  [ 1728/60000]\n",
      "loss: 24.027790  [ 1760/60000]\n",
      "loss: 22.368350  [ 1792/60000]\n",
      "loss: 21.205381  [ 1824/60000]\n",
      "loss: 15.998649  [ 1856/60000]\n",
      "loss: 15.583522  [ 1888/60000]\n",
      "loss: 16.879851  [ 1920/60000]\n"
     ]
    }
   ],
   "execution_count": null,
   "source": [
    "batch = 0\n",
    "size = len(img_train)\n",
    "for data_train in data:\n",
    "    sgd.zero_grad()\n",
    "    img = data_train[0]\n",
    "    label = data_train[1]\n",
    "    output = net(img)\n",
    "    loss = nn.functional.cross_entropy_loss(output, label) + net.l2normalization()\n",
    "    loss.backward()\n",
    "    sgd.step()\n",
    "    batch += 1\n",
    "    losses32.append(loss.item())\n",
    "    print(f\"loss: {loss:>7f}  [{batch * BATCH_SIZE:>5d}/{size:>5d}]\")"
   ],
   "id": "1e18ff1d6f0de4cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ea0eae5689f3783a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置样式为 'ggplot'\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# 创建一个新的图像，并设置其大小\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 绘制损失值，设置线的宽度为 1\n",
    "plt.plot(losses32, linewidth=0.5)\n",
    "\n",
    "# 设置图像的标题和坐标轴标签\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# 显示图像"
   ],
   "id": "8e696cc2063d80a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "a = np.eye(0)\n",
    "a"
   ],
   "id": "805ef1849f2dcb7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "55f41ac00386d954",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
